{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88692b0b",
   "metadata": {},
   "source": [
    "# Importamos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28b92cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos train.csv y test.csv...\n",
      "Datos cargados correctamente.\n",
      "\n",
      "Primeras 5 filas de df_entrenamiento_bruto:\n",
      "      id            timestamp modo_operacion operador  temperatura    presion  \\\n",
      "0  15000  2021-04-24 03:00:00  mantenimiento      NaN    20.428941  58.474497   \n",
      "1  15001  2020-09-26 19:00:00  mantenimiento        E     1.332931  84.784184   \n",
      "2  15002  2021-07-02 15:00:00           auto        D    17.501579  65.057682   \n",
      "3  15003  2020-02-05 22:00:00           auto      NaN     8.118801  70.127615   \n",
      "4  15004  2020-09-05 18:00:00         manual      NaN    -0.353122  90.738090   \n",
      "\n",
      "   sensor_ruido   sensor_3  fallo  \n",
      "0      0.349400  18.301299      0  \n",
      "1     -2.252684   7.295245      0  \n",
      "2      0.359853  34.271305      0  \n",
      "3      0.489166  -3.748938      0  \n",
      "4     -1.505866  10.157637      0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import category_encoders as ce # Para Target Encoding si se usa\n",
    "import itertools # Para generar combinaciones de características\n",
    "import warnings\n",
    "\n",
    "# Ignorar warnings para mantener la salida limpia\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Cargar datos\n",
    "print(\"Cargando datos train.csv y test.csv...\")\n",
    "try:\n",
    "    df_entrenamiento_bruto = pd.read_csv('train.csv')\n",
    "    df_prueba_bruto = pd.read_csv('test.csv')\n",
    "    print(\"Datos cargados correctamente.\")\n",
    "    print(\"\\nPrimeras 5 filas de df_entrenamiento_bruto:\")\n",
    "    print(df_entrenamiento_bruto.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Asegúrate de que 'train.csv' y 'test.csv' estén en el directorio correcto.\")\n",
    "    df_entrenamiento_bruto = pd.DataFrame() \n",
    "    df_prueba_bruto = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a89dc438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Variables Globales Definidas ---\n",
      "  Objetivo: fallo\n",
      "  Características numéricas base: ['temperatura', 'presion', 'sensor_ruido', 'sensor_3']\n",
      "  Características categóricas: ['modo_operacion', 'operador']\n",
      "  Columna ID a eliminar: id\n",
      "  Columna Timestamp a procesar: timestamp\n",
      "\n",
      "NaNs en df_entrenamiento_bruto:\n",
      "id                   0\n",
      "timestamp            0\n",
      "modo_operacion       0\n",
      "operador          2731\n",
      "temperatura          0\n",
      "presion            625\n",
      "sensor_ruido         0\n",
      "sensor_3             0\n",
      "fallo                0\n",
      "dtype: int64\n",
      "\n",
      "NaNs en df_prueba_bruto:\n",
      "id                   0\n",
      "timestamp            0\n",
      "modo_operacion       0\n",
      "operador          2700\n",
      "temperatura          0\n",
      "presion            625\n",
      "sensor_ruido         0\n",
      "sensor_3             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "objetivo = 'fallo'\n",
    "\n",
    "columnas_numericas_base = ['temperatura', 'presion', 'sensor_ruido', 'sensor_3']\n",
    "columnas_categoricas = ['modo_operacion', 'operador'] \n",
    "columna_id = 'id' # Estan aparte para poder tratarlas ahora\n",
    "columna_timestamp = 'timestamp'\n",
    "\n",
    "print(\"--- Variables Globales Definidas ---\")\n",
    "print(f\"  Objetivo: {objetivo}\")\n",
    "print(f\"  Características numéricas base: {columnas_numericas_base}\")\n",
    "print(f\"  Características categóricas: {columnas_categoricas}\")\n",
    "print(f\"  Columna ID a eliminar: {columna_id}\")\n",
    "print(f\"  Columna Timestamp a procesar: {columna_timestamp}\")\n",
    "\n",
    "# Mostrar NaNs\n",
    "print(\"\\nNaNs en df_entrenamiento_bruto:\")\n",
    "print(df_entrenamiento_bruto.isnull().sum())\n",
    "print(\"\\nNaNs en df_prueba_bruto:\")\n",
    "print(df_prueba_bruto.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec400d",
   "metadata": {},
   "source": [
    "# Preparamos Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b53db7b",
   "metadata": {},
   "source": [
    "## Columnas id y TimeStamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10a65e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Eliminando 'id' de df_entrenamiento.\n",
      "  Eliminando 'id' de df_prueba.\n",
      "\n",
      "Nuevas características de tiempo generadas: ['hora', 'dia_semana', 'mes', 'dia_del_anio', 'es_fin_de_semana']\n",
      "Todas las características numéricas para imputación serán: ['temperatura', 'presion', 'sensor_ruido', 'sensor_3', 'hora', 'dia_semana', 'mes', 'dia_del_anio', 'es_fin_de_semana']\n",
      "  modo_operacion operador  temperatura    presion  sensor_ruido   sensor_3  \\\n",
      "0  mantenimiento      NaN    20.428941  58.474497      0.349400  18.301299   \n",
      "1  mantenimiento        E     1.332931  84.784184     -2.252684   7.295245   \n",
      "2           auto        D    17.501579  65.057682      0.359853  34.271305   \n",
      "3           auto      NaN     8.118801  70.127615      0.489166  -3.748938   \n",
      "4         manual      NaN    -0.353122  90.738090     -1.505866  10.157637   \n",
      "\n",
      "   fallo  hora  dia_semana  mes  dia_del_anio  es_fin_de_semana  \n",
      "0      0     3           5    4           114                 1  \n",
      "1      0    19           5    9           270                 1  \n",
      "2      0    15           4    7           183                 0  \n",
      "3      0    22           2    2            36                 0  \n",
      "4      0    18           5    9           249                 1  \n"
     ]
    }
   ],
   "source": [
    "# Crear copias de trabajo\n",
    "df_entrenamiento = df_entrenamiento_bruto.copy()\n",
    "df_prueba = df_prueba_bruto.copy()\n",
    "\n",
    "print(f\"  Eliminando '{columna_id}' de df_entrenamiento.\")\n",
    "df_entrenamiento = df_entrenamiento.drop(columns=[columna_id])\n",
    "print(f\"  Eliminando '{columna_id}' de df_prueba.\")\n",
    "df_prueba = df_prueba.drop(columns=[columna_id])\n",
    "\n",
    "\n",
    "def dividirTimeStamp(df, nombre_columna):    \n",
    "    df[nombre_columna] = pd.to_datetime(df[nombre_columna])\n",
    "\n",
    "    df['hora'] = df[nombre_columna].dt.hour\n",
    "    df['dia_semana'] = df[nombre_columna].dt.dayofweek # Lunes=0, Domingo=6\n",
    "    df['mes'] = df[nombre_columna].dt.month\n",
    "    df['dia_del_anio'] = df[nombre_columna].dt.dayofyear\n",
    "    df['es_fin_de_semana'] = (df[nombre_columna].dt.dayofweek >= 5).astype(int) \n",
    "    \n",
    "    caracteristicas_generadas = ['hora', 'dia_semana', 'mes', 'dia_del_anio', 'es_fin_de_semana']\n",
    "    \n",
    "    # Se elimina la columna del df original\n",
    "    df = df.drop(columns=[nombre_columna])\n",
    "    return df, caracteristicas_generadas\n",
    "\n",
    "df_entrenamiento, caracteristicas_tiempo_generadas_train = dividirTimeStamp(df_entrenamiento, columna_timestamp)\n",
    "df_prueba, _ = dividirTimeStamp(df_prueba, columna_timestamp) # No necesitamos la lista de test, asumimos que es la misma\n",
    "\n",
    "todas_columnas_numericas = columnas_numericas_base + caracteristicas_tiempo_generadas_train\n",
    "\n",
    "print(f\"\\nNuevas características de tiempo generadas: {caracteristicas_tiempo_generadas_train}\")\n",
    "print(f\"Todas las características numéricas para imputación serán: {todas_columnas_numericas}\")\n",
    "print(df_entrenamiento.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e29951c",
   "metadata": {},
   "source": [
    "## Quitar los Nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5e932b",
   "metadata": {},
   "source": [
    "### En las Numericas\n",
    "Hay dos opciones: \n",
    "\n",
    "Imputar con la Mediana, se entrena a un modelo y se sustituye los Nan con la prediccion. Es buena pero no siempre fiable\n",
    "\n",
    "Imputar con KNNImputer, una version mas avanzada de imputar con la mediana. El problema es que hay que escalar los datos antes de pasarlos por el Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f214c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entrenamiento_procesado = df_entrenamiento.copy()\n",
    "df_prueba_procesado = df_prueba.copy()\n",
    "\n",
    "# Columnas numéricas existentes en los dataframes para imputar\n",
    "columnas_numericas_a_imputar_train = [col for col in todas_columnas_numericas if col in df_entrenamiento_procesado.columns]\n",
    "columnas_numericas_a_imputar_test = [col for col in todas_columnas_numericas if col in df_prueba_procesado.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0620253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimputador_mediana = SimpleImputer(strategy='median')\\ndf_entrenamiento_procesado[columnas_numericas_a_imputar_train] = imputador_mediana.fit_transform(df_entrenamiento_procesado[columnas_numericas_a_imputar_train])\\n\\ncols_comunes_test = [col for col in columnas_numericas_a_imputar_test if col in imputador_mediana.feature_names_in_]\\ndf_prueba_procesado[cols_comunes_test] = imputador_mediana.transform(df_prueba_procesado[cols_comunes_test])\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imputacion de la mediana ==> Peor || Comentado para que no ejecute\n",
    "\"\"\"\n",
    "imputador_mediana = SimpleImputer(strategy='median')\n",
    "df_entrenamiento_procesado[columnas_numericas_a_imputar_train] = imputador_mediana.fit_transform(df_entrenamiento_procesado[columnas_numericas_a_imputar_train])\n",
    "\n",
    "cols_comunes_test = [col for col in columnas_numericas_a_imputar_test if col in imputador_mediana.feature_names_in_]\n",
    "df_prueba_procesado[cols_comunes_test] = imputador_mediana.transform(df_prueba_procesado[cols_comunes_test])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb9e0771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNNImputer ==> La que mejor va\n",
    "escalador_knn = StandardScaler() \n",
    "\n",
    "# Para poder usar KNNImputer, hace falta escalar los datos\n",
    "df_entrenamiento_escalado_num = escalador_knn.fit_transform(df_entrenamiento_procesado[columnas_numericas_a_imputar_train])\n",
    "\n",
    "imputador_knn = KNNImputer(n_neighbors=5) \n",
    "df_entrenamiento_imputado_escalado = imputador_knn.fit_transform(df_entrenamiento_escalado_num) # Fit y transform en train escalado\n",
    "\n",
    "# Desescalar train\n",
    "df_entrenamiento_procesado[columnas_numericas_a_imputar_train] = escalador_knn.inverse_transform(df_entrenamiento_imputado_escalado)\n",
    "\n",
    "# Transformar test\n",
    "cols_comunes_test_knn = [col for col in columnas_numericas_a_imputar_test if col in escalador_knn.feature_names_in_]\n",
    "df_prueba_escalado_num = escalador_knn.transform(df_prueba_procesado[cols_comunes_test_knn])\n",
    "df_prueba_imputado_escalado = imputador_knn.transform(df_prueba_escalado_num) # Solo transform en test escalado\n",
    "df_prueba_procesado[cols_comunes_test_knn] = escalador_knn.inverse_transform(df_prueba_imputado_escalado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dc453a",
   "metadata": {},
   "source": [
    "### En las categóricas\n",
    "Aqui simplemente reemplazamos los Nan por un nuevo valor llamado \"Desconocido\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "430a8414",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_categoricas_a_imputar_train = [col for col in columnas_categoricas if col in df_entrenamiento_procesado.columns]\n",
    "columnas_categoricas_a_imputar_test = [col for col in columnas_categoricas if col in df_prueba_procesado.columns]\n",
    "\n",
    "imputador_constante_cat = SimpleImputer(strategy='constant', fill_value='Faltante') # Traducido \"Missing\"\n",
    "\n",
    "# Ajustar y transformar train\n",
    "df_entrenamiento_procesado[columnas_categoricas_a_imputar_train] = imputador_constante_cat.fit_transform(df_entrenamiento_procesado[columnas_categoricas_a_imputar_train])\n",
    "\n",
    "# Transformar test\n",
    "cols_comunes_test_cat = [col for col in columnas_categoricas_a_imputar_test if col in imputador_constante_cat.feature_names_in_]\n",
    "df_prueba_procesado[cols_comunes_test_cat] = imputador_constante_cat.transform(df_prueba_procesado[cols_comunes_test_cat])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d70d80d",
   "metadata": {},
   "source": [
    "### Comprobacion de los Nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5edc1601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comprobación de NaNs después de toda la imputación:\n",
      "NaNs en df_entrenamiento_procesado (solo columnas con NaNs):\n",
      "Series([], dtype: int64)\n",
      "\n",
      "NaNs en df_prueba_procesado (solo columnas con NaNs):\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComprobación de NaNs después de toda la imputación:\")\n",
    "print(\"NaNs en df_entrenamiento_procesado (solo columnas con NaNs):\")\n",
    "print(df_entrenamiento_procesado.isnull().sum().loc[lambda x: x > 0])\n",
    "print(\"\\nNaNs en df_prueba_procesado (solo columnas con NaNs):\")\n",
    "print(df_prueba_procesado.isnull().sum().loc[lambda x: x > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf8eaaa",
   "metadata": {},
   "source": [
    "## Tratar las categoricas para los modelos\n",
    "Vamos a probar dos, OneHot y Target Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d123bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_entrenamiento_final y df_prueba_final serán el resultado de esta celda\n",
    "df_entrenamiento_final = df_entrenamiento_procesado.copy()\n",
    "df_prueba_final = df_prueba_procesado.copy()\n",
    "\n",
    "# Columnas categóricas existentes para la codificación\n",
    "columnas_categoricas_a_codificar_train = [col for col in columnas_categoricas if col in df_entrenamiento_final.columns]\n",
    "columnas_categoricas_a_codificar_test = [col for col in columnas_categoricas if col in df_prueba_final.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b3d6d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncodificador_ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore', drop='first')\\n\\n# Ajustar y transformar train\\nmatriz_codificada_entrenamiento = codificador_ohe.fit_transform(df_entrenamiento_final[columnas_categoricas_a_codificar_train])\\ndf_codificado_entrenamiento = pd.DataFrame(matriz_codificada_entrenamiento, \\n                                            columns=codificador_ohe.get_feature_names_out(columnas_categoricas_a_codificar_train), \\n                                            index=df_entrenamiento_final.index)\\n\\n# Eliminar categóricas originales y unir las codificadas\\ndf_entrenamiento_final = df_entrenamiento_final.drop(columns=columnas_categoricas_a_codificar_train)\\ndf_entrenamiento_final = df_entrenamiento_final.join(df_codificado_entrenamiento)\\n\\n\\nmatriz_codificada_prueba = codificador_ohe.transform(df_prueba_final[columnas_categoricas_a_codificar_test])\\ndf_codificado_prueba = pd.DataFrame(matriz_codificada_prueba, \\n                                    columns=codificador_ohe.get_feature_names_out(columnas_categoricas_a_codificar_test), \\n                                    index=df_prueba_final.index)\\ndf_prueba_final = df_prueba_final.drop(columns=columnas_categoricas_a_codificar_test)\\ndf_prueba_final = df_prueba_final.join(df_codificado_prueba)\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OneHot ==> Peor || Comentado para que no ejecute\n",
    "\"\"\"\n",
    "codificador_ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore', drop='first')\n",
    "\n",
    "# Ajustar y transformar train\n",
    "matriz_codificada_entrenamiento = codificador_ohe.fit_transform(df_entrenamiento_final[columnas_categoricas_a_codificar_train])\n",
    "df_codificado_entrenamiento = pd.DataFrame(matriz_codificada_entrenamiento, \n",
    "                                            columns=codificador_ohe.get_feature_names_out(columnas_categoricas_a_codificar_train), \n",
    "                                            index=df_entrenamiento_final.index)\n",
    "\n",
    "# Eliminar categóricas originales y unir las codificadas\n",
    "df_entrenamiento_final = df_entrenamiento_final.drop(columns=columnas_categoricas_a_codificar_train)\n",
    "df_entrenamiento_final = df_entrenamiento_final.join(df_codificado_entrenamiento)\n",
    "\n",
    "\n",
    "matriz_codificada_prueba = codificador_ohe.transform(df_prueba_final[columnas_categoricas_a_codificar_test])\n",
    "df_codificado_prueba = pd.DataFrame(matriz_codificada_prueba, \n",
    "                                    columns=codificador_ohe.get_feature_names_out(columnas_categoricas_a_codificar_test), \n",
    "                                    index=df_prueba_final.index)\n",
    "df_prueba_final = df_prueba_final.drop(columns=columnas_categoricas_a_codificar_test)\n",
    "df_prueba_final = df_prueba_final.join(df_codificado_prueba)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3770e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target encoder ==> Mejor\n",
    "codificador_objetivo = ce.TargetEncoder(cols=columnas_categoricas_a_codificar_train, smoothing=10.0)\n",
    "\n",
    "# Ajustar y transformar train (modifica el DataFrame directamente)\n",
    "df_entrenamiento_final[columnas_categoricas_a_codificar_train] = codificador_objetivo.fit_transform(df_entrenamiento_final[columnas_categoricas_a_codificar_train], df_entrenamiento_final[objetivo])\n",
    "\n",
    "# Transformar test\n",
    "df_prueba_final[columnas_categoricas_a_codificar_test] = codificador_objetivo.transform(df_prueba_final[columnas_categoricas_a_codificar_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8225c8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Forma de df_entrenamiento_final (después de codificación categórica): (15000, 12)\n",
      "Tipos de datos en df_entrenamiento_final:\n",
      "modo_operacion      float64\n",
      "operador            float64\n",
      "temperatura         float64\n",
      "presion             float64\n",
      "sensor_ruido        float64\n",
      "sensor_3            float64\n",
      "fallo                 int64\n",
      "hora                float64\n",
      "dia_semana          float64\n",
      "mes                 float64\n",
      "dia_del_anio        float64\n",
      "es_fin_de_semana    float64\n",
      "dtype: object\n",
      "\n",
      "Verificación final de NaNs en df_entrenamiento_final (deberían ser 0):\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nForma de df_entrenamiento_final (después de codificación categórica): {df_entrenamiento_final.shape if 'df_entrenamiento_final' in locals() else 'No definido'}\")\n",
    "if 'df_entrenamiento_final' in locals() and not df_entrenamiento_final.empty:\n",
    "    print(\"Tipos de datos en df_entrenamiento_final:\")\n",
    "    print(df_entrenamiento_final.dtypes)\n",
    "    print(\"\\nVerificación final de NaNs en df_entrenamiento_final (deberían ser 0):\")\n",
    "    print(df_entrenamiento_final.isnull().sum().loc[lambda x: x > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d42bbaa",
   "metadata": {},
   "source": [
    "## Datos sin Tocar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88a50e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entrenamiento_base = df_entrenamiento_final.copy()\n",
    "df_prueba_base = df_prueba_final.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70344827",
   "metadata": {},
   "source": [
    "## Datos quitando las no prometedoras (Sensor Ruido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebccf481",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entrenamiento_reducido = df_entrenamiento_final.copy()\n",
    "df_prueba_reducido = df_prueba_final.copy()\n",
    "columna_a_quitar = 'sensor_ruido' # Es la unica que no aumenta la informacion, o la que parece al menos que no\n",
    "\n",
    "df_entrenamiento_reducido = df_entrenamiento_reducido.drop(columns=[columna_a_quitar])\n",
    "df_prueba_reducido = df_prueba_reducido.drop(columns=[columna_a_quitar])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675d914e",
   "metadata": {},
   "source": [
    "## Datos relacionados TODOS con TODOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32d02f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entrenamiento_interacciones = df_entrenamiento_final.copy()\n",
    "df_prueba_interacciones = df_prueba_final.copy()\n",
    "\n",
    "\n",
    "for col1, col2 in itertools.combinations(columnas_numericas_base, 2): # Mejor un paquete para las iteraciones :)\n",
    "\n",
    "    nombre_col_interaccion = f\"{col1}_x_{col2}\" \n",
    "    df_entrenamiento_interacciones[nombre_col_interaccion] = df_entrenamiento_interacciones[col1] * df_entrenamiento_interacciones[col2]\n",
    "    \n",
    "    df_prueba_interacciones[nombre_col_interaccion] = df_prueba_interacciones[col1] * df_prueba_interacciones[col2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2265d7a",
   "metadata": {},
   "source": [
    "# Probar modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fe00ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import make_scorer, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "027dc178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Iniciando Evaluación de Modelos ---\n",
      "\n",
      "======================================================================\n",
      "--- Evaluando Conjunto de Datos: Base (Forma: (15000, 12)) ---\n",
      "======================================================================\n",
      "\n",
      "  --- Modelo: Árbol de Decisión ---\n",
      "    F1-Score Promedio (4 folds): 0.9223 (std: 0.0029)\n",
      "    F1-Scores por fold: [np.float64(0.922), np.float64(0.9202), np.float64(0.9199), np.float64(0.9271)]\n",
      "\n",
      "  --- Modelo: Naive Bayes Gaussiano ---\n",
      "    F1-Score Promedio (4 folds): 0.7214 (std: 0.0111)\n",
      "    F1-Scores por fold: [np.float64(0.7061), np.float64(0.7351), np.float64(0.7161), np.float64(0.7283)]\n",
      "\n",
      "  --- Modelo: Random Forest ---\n",
      "    F1-Score Promedio (4 folds): 0.9485 (std: 0.0060)\n",
      "    F1-Scores por fold: [np.float64(0.9436), np.float64(0.951), np.float64(0.9422), np.float64(0.9571)]\n",
      "\n",
      "  --- Modelo: Gradient Boosting ---\n",
      "    F1-Score Promedio (4 folds): 0.9465 (std: 0.0067)\n",
      "    F1-Scores por fold: [np.float64(0.9384), np.float64(0.9476), np.float64(0.9435), np.float64(0.9567)]\n",
      "\n",
      "  --- Modelo: Bagging ---\n",
      "    F1-Score Promedio (4 folds): 0.9446 (std: 0.0044)\n",
      "    F1-Scores por fold: [np.float64(0.9402), np.float64(0.946), np.float64(0.9411), np.float64(0.9511)]\n",
      "\n",
      "======================================================================\n",
      "--- Evaluando Conjunto de Datos: Caracteristicas Reducidas (Forma: (15000, 11)) ---\n",
      "======================================================================\n",
      "\n",
      "  --- Modelo: Árbol de Decisión ---\n",
      "    F1-Score Promedio (4 folds): 0.9273 (std: 0.0039)\n",
      "    F1-Scores por fold: [np.float64(0.9263), np.float64(0.9231), np.float64(0.926), np.float64(0.9337)]\n",
      "\n",
      "  --- Modelo: Naive Bayes Gaussiano ---\n",
      "    F1-Score Promedio (4 folds): 0.7214 (std: 0.0110)\n",
      "    F1-Scores por fold: [np.float64(0.7065), np.float64(0.7355), np.float64(0.7161), np.float64(0.7276)]\n",
      "\n",
      "  --- Modelo: Random Forest ---\n",
      "    F1-Score Promedio (4 folds): 0.9514 (std: 0.0045)\n",
      "    F1-Scores por fold: [np.float64(0.9471), np.float64(0.9534), np.float64(0.9471), np.float64(0.9578)]\n",
      "\n",
      "  --- Modelo: Gradient Boosting ---\n",
      "    F1-Score Promedio (4 folds): 0.9488 (std: 0.0056)\n",
      "    F1-Scores por fold: [np.float64(0.9407), np.float64(0.9529), np.float64(0.9468), np.float64(0.955)]\n",
      "\n",
      "  --- Modelo: Bagging ---\n",
      "    F1-Score Promedio (4 folds): 0.9468 (std: 0.0018)\n",
      "    F1-Scores por fold: [np.float64(0.9457), np.float64(0.945), np.float64(0.9465), np.float64(0.9498)]\n",
      "\n",
      "======================================================================\n",
      "--- Evaluando Conjunto de Datos: Interaccion de Caracteristicas (Forma: (15000, 18)) ---\n",
      "======================================================================\n",
      "\n",
      "  --- Modelo: Árbol de Decisión ---\n",
      "    F1-Score Promedio (4 folds): 0.9216 (std: 0.0039)\n",
      "    F1-Scores por fold: [np.float64(0.9192), np.float64(0.9192), np.float64(0.9199), np.float64(0.9283)]\n",
      "\n",
      "  --- Modelo: Naive Bayes Gaussiano ---\n",
      "    F1-Score Promedio (4 folds): 0.6661 (std: 0.0101)\n",
      "    F1-Scores por fold: [np.float64(0.655), np.float64(0.6676), np.float64(0.6602), np.float64(0.6817)]\n",
      "\n",
      "  --- Modelo: Random Forest ---\n",
      "    F1-Score Promedio (4 folds): 0.9318 (std: 0.0081)\n",
      "    F1-Scores por fold: [np.float64(0.9257), np.float64(0.9346), np.float64(0.9231), np.float64(0.9438)]\n",
      "\n",
      "  --- Modelo: Gradient Boosting ---\n",
      "    F1-Score Promedio (4 folds): 0.9476 (std: 0.0071)\n",
      "    F1-Scores por fold: [np.float64(0.9374), np.float64(0.9511), np.float64(0.9453), np.float64(0.9567)]\n",
      "\n",
      "  --- Modelo: Bagging ---\n",
      "    F1-Score Promedio (4 folds): 0.9431 (std: 0.0047)\n",
      "    F1-Scores por fold: [np.float64(0.9359), np.float64(0.9466), np.float64(0.942), np.float64(0.9479)]\n",
      "\n",
      "\n",
      "======================================================================\n",
      "--- Tabla Resumen de F1-Score Promedio (clase positiva=1) ---\n",
      "======================================================================\n",
      "                                Árbol de Decisión  Naive Bayes Gaussiano  Random Forest  Gradient Boosting  Bagging\n",
      "Base                                       0.9223                 0.7214         0.9485             0.9465   0.9446\n",
      "Caracteristicas Reducidas                  0.9273                 0.7214         0.9514             0.9488   0.9468\n",
      "Interaccion de Caracteristicas             0.9216                 0.6661         0.9318             0.9476   0.9431\n"
     ]
    }
   ],
   "source": [
    "# --- Bucle de Entrenamiento y Evaluación de Modelos ---\n",
    "print(\"\\n--- Iniciando Evaluación de Modelos ---\")\n",
    "\n",
    "conjuntos_datos_a_evaluar = []\n",
    "\n",
    "conjuntos_datos_a_evaluar.append({'nombre': 'Base', 'datos': df_entrenamiento_base})\n",
    "conjuntos_datos_a_evaluar.append({'nombre': 'Caracteristicas Reducidas', 'datos': df_entrenamiento_reducido})\n",
    "conjuntos_datos_a_evaluar.append({'nombre': 'Interaccion de Caracteristicas', 'datos': df_entrenamiento_interacciones})\n",
    "\n",
    "# Preparar lista de modelos\n",
    "modelos_a_evaluar = [\n",
    "    ('Árbol de Decisión', DecisionTreeClassifier(random_state=42)), # Traducido\n",
    "    ('Naive Bayes Gaussiano', GaussianNB()), # Traducido\n",
    "    ('Random Forest', RandomForestClassifier(random_state=42, n_jobs=-1)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=42)),\n",
    "    ('Bagging', BaggingClassifier(random_state=42, n_jobs=-1))\n",
    "]\n",
    "\n",
    "# Definir StratifiedKFold y el F1 scorer\n",
    "evaluador_f1 = make_scorer(f1_score, pos_label=1, zero_division=0)\n",
    "num_divisiones = 4 \n",
    "kfold_estratificado = StratifiedKFold(n_splits=num_divisiones, shuffle=True, random_state=42) \n",
    "\n",
    "resultados_completos = {} \n",
    "resumen_f1_promedio = {} \n",
    "\n",
    "for info_conjunto in conjuntos_datos_a_evaluar: \n",
    "    nombre_conjunto = info_conjunto['nombre'] \n",
    "    df_actual_entrenamiento = info_conjunto['datos'] \n",
    "    \n",
    "    print(f\"\\n======================================================================\")\n",
    "    print(f\"--- Evaluando Conjunto de Datos: {nombre_conjunto} (Forma: {df_actual_entrenamiento.shape}) ---\")\n",
    "    print(f\"======================================================================\")\n",
    "\n",
    "    if objetivo not in df_actual_entrenamiento.columns:\n",
    "        print(f\"  ERROR: La columna objetivo '{objetivo}' no se encuentra en '{nombre_conjunto}'. Omitiendo.\")\n",
    "        continue\n",
    "\n",
    "    X_entrenamiento = df_actual_entrenamiento.drop(columns=[objetivo]) \n",
    "    y_entrenamiento = df_actual_entrenamiento[objetivo] \n",
    "    \n",
    "    \n",
    "    resumen_f1_promedio[nombre_conjunto] = {}\n",
    "    resultados_completos[nombre_conjunto] = {}\n",
    "\n",
    "    for nombre_modelo, instancia_modelo in modelos_a_evaluar: \n",
    "        print(f\"\\n  --- Modelo: {nombre_modelo} ---\")\n",
    "        try:\n",
    "            puntuaciones_cv_f1 = cross_val_score(instancia_modelo, X_entrenamiento, y_entrenamiento, cv=kfold_estratificado, scoring=evaluador_f1, n_jobs=-1) \n",
    "            \n",
    "            f1_promedio_modelo = np.mean(puntuaciones_cv_f1) \n",
    "            desv_est_f1_modelo = np.std(puntuaciones_cv_f1) \n",
    "            \n",
    "            resumen_f1_promedio[nombre_conjunto][nombre_modelo] = f1_promedio_modelo\n",
    "            resultados_completos[nombre_conjunto][nombre_modelo] = {'f1_promedio': f1_promedio_modelo, 'desv_est_f1': desv_est_f1_modelo, 'f1_por_fold': puntuaciones_cv_f1.tolist()}\n",
    "            \n",
    "            print(f\"    F1-Score Promedio ({num_divisiones} folds): {f1_promedio_modelo:.4f} (std: {desv_est_f1_modelo:.4f})\")\n",
    "            print(f\"    F1-Scores por fold: {[round(s, 4) for s in puntuaciones_cv_f1]}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ERROR al entrenar/evaluar {nombre_modelo} en {nombre_conjunto}: {e}\")\n",
    "            resumen_f1_promedio[nombre_conjunto][nombre_modelo] = np.nan\n",
    "            resultados_completos[nombre_conjunto][nombre_modelo] = {'f1_promedio': np.nan, 'desv_est_f1': np.nan, 'f1_por_fold': [], 'error': str(e)}\n",
    "\n",
    "# Imprimir tabla resumen\n",
    "print(f\"\\n\\n======================================================================\")\n",
    "print(f\"--- Tabla Resumen de F1-Score Promedio (clase positiva=1) ---\")\n",
    "print(f\"======================================================================\")\n",
    "\n",
    "\n",
    "df_resumen = pd.DataFrame(resumen_f1_promedio).T \n",
    "orden_modelos = [name for name, _ in modelos_a_evaluar] \n",
    "columnas_existentes_modelos = [m for m in orden_modelos if m in df_resumen.columns] \n",
    "if columnas_existentes_modelos:\n",
    "    df_resumen = df_resumen[columnas_existentes_modelos]\n",
    "\n",
    "print(df_resumen.to_string(float_format=\"%.4f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69617b42",
   "metadata": {},
   "source": [
    "## Funcion Predict Modelo\n",
    "Esta funcion, dado un modelo y una df hace un predict y lo guarda en su correspondiente csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96b472c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictCSV(model, model_name, df_toPredict, df_name):\n",
    "    columns = ['id', 'fallo']\n",
    "    csv_file = f\"CSVModelos/{model_name}_{df_name}.csv\"\n",
    "    header = True\n",
    "\n",
    "    for i in range(0, len(df_toPredict), 500):\n",
    "        test_predict = model.predict(df_toPredict[i:i+500])\n",
    "        \n",
    "        indi = [j for j in range(i, i+500,1)]\n",
    "        valor = [test_predict[k] for k in range(500)]\n",
    "\n",
    "        results = pd.DataFrame(list(zip(indi,valor)), columns=columns)\n",
    "\n",
    "        results.to_csv(csv_file, mode='a', header=header, index=False)\n",
    "        header = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1e41fa",
   "metadata": {},
   "source": [
    "## DECISIONTREE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7559a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictArbolDecision():\n",
    "    df_to_train = df_entrenamiento_reducido.copy()\n",
    "    df_toTest = df_prueba_reducido.copy()\n",
    "\n",
    "    X_entrenamiento = df_to_train.drop(columns=[objetivo]) \n",
    "    y_entrenamiento = df_to_train[objetivo] \n",
    "\n",
    "    model = DecisionTreeClassifier(random_state=42)\n",
    "    model.fit(X_entrenamiento,y_entrenamiento)\n",
    "\n",
    "\n",
    "    predictCSV(model,\"ArbolDecision\",df_toTest,\"Reducido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d81a0cd",
   "metadata": {},
   "source": [
    "## NAIVEBAYES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afac54c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictNaiveBayes():\n",
    "    df_to_train = df_entrenamiento_base.copy()\n",
    "    df_toTest = df_prueba_base.copy()\n",
    "\n",
    "    X_entrenamiento = df_to_train.drop(columns=[objetivo]) \n",
    "    y_entrenamiento = df_to_train[objetivo] \n",
    "\n",
    "    model = GaussianNB()\n",
    "    model.fit(X_entrenamiento,y_entrenamiento)\n",
    "\n",
    "\n",
    "    predictCSV(model,\"NaiveBayes\",df_toTest,\"Basico\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4263a810",
   "metadata": {},
   "source": [
    "## RANDOMFOREST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b64c26d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictRandomForest():\n",
    "    df_to_train = df_entrenamiento_reducido.copy()\n",
    "    df_toTest = df_prueba_reducido.copy()\n",
    "\n",
    "    X_entrenamiento = df_to_train.drop(columns=[objetivo]) \n",
    "    y_entrenamiento = df_to_train[objetivo] \n",
    "\n",
    "    model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    model.fit(X_entrenamiento,y_entrenamiento)\n",
    "\n",
    "\n",
    "    predictCSV(model,\"RandomForest\",df_toTest,\"Reducido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628fde86",
   "metadata": {},
   "source": [
    "## GRADIENTBOOSTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8290927",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predictGradientBoosting():\n",
    "    df_to_train = df_entrenamiento_base.copy()\n",
    "    df_toTest = df_prueba_base.copy()\n",
    "\n",
    "    X_entrenamiento = df_to_train.drop(columns=[objetivo]) \n",
    "    y_entrenamiento = df_to_train[objetivo] \n",
    "\n",
    "    model = GradientBoostingClassifier(random_state=42)\n",
    "    model.fit(X_entrenamiento,y_entrenamiento)\n",
    "\n",
    "\n",
    "    predictCSV(model,\"GradientBoosting\",df_toTest,\"Basico\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08206b80",
   "metadata": {},
   "source": [
    "## BAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "103cf294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictBagging():\n",
    "    df_to_train = df_entrenamiento_reducido.copy()\n",
    "    df_toTest = df_prueba_reducido.copy()\n",
    "\n",
    "    X_entrenamiento = df_to_train.drop(columns=[objetivo]) \n",
    "    y_entrenamiento = df_to_train[objetivo] \n",
    "\n",
    "    model = BaggingClassifier(random_state=42, n_jobs=-1)\n",
    "    model.fit(X_entrenamiento,y_entrenamiento)\n",
    "\n",
    "\n",
    "    predictCSV(model,\"ArbolDecision\",df_toTest,\"Reducido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04b31061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictArbolDecision()\n",
    "#predictNaiveBayes()\n",
    "#predictRandomForest()\n",
    "#predictGradientBoosting()\n",
    "#predictBagging()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UNI312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
